{"cells":[{"cell_type":"markdown","metadata":{"id":"hycKlgDEpQdR"},"source":["Fine-Tuning TimeLMLARGE for the Task of Emoji Prediction\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fTawgIz4b2ZZ"},"outputs":[],"source":["import pandas as pd\n","import re\n","from transformers import AutoTokenizer, AutoModelForMaskedLM\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch\n","from transformers import RobertaForSequenceClassification, AdamW, get_scheduler\n","import torch.nn.functional as F\n","from transformers import AutoModelForSequenceClassification\n","\n","# Define file paths\n","filepath = \"\"\n","train_path = filepath + \"emoji-train.csv\"\n","validation_path = filepath + \"emoji-validation.csv\"\n","test_path = filepath + \"emoji-test.csv\"\n","\n","# Load datasets\n","train_data = pd.read_csv(train_path)\n","validation_data = pd.read_csv(validation_path)\n","test_data = pd.read_csv(test_path)\n","\n","# Display the first few rows of each dataset\n","# print(\"Training Dataset:\")\n","# print(train_data.head())\n","# print(\"\\nValidation Dataset:\")\n","# print(validation_data.head())\n","# print(\"\\nTest Dataset:\")\n","# print(test_data.head())\n"]},{"cell_type":"markdown","metadata":{"id":"c4bt6ZfupvZu"},"source":["To remove the noise before training, some preprocessing steps should ne taken:\n","\n","\n","\n","1.   Removing the URLs\n","2.   Removing the mentions\n","3. removing the special characters\n","\n","this is done by The Regular Expression Library\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7hGOF1w2iyh4"},"outputs":[],"source":["# Preprocessing function to clean the text\n","def preprocess_text(text):\n","    # Remove URLs\n","    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n","    # Remove mentions\n","    text = re.sub(r\"@\\w+\", \"\", text)\n","    # Remove special characters\n","    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n","    return text\n","\n","# Apply preprocessing\n","train_data['cleaned_text'] = train_data['text'].apply(preprocess_text)\n","validation_data['cleaned_text'] = validation_data['text'].apply(preprocess_text)\n","test_data['cleaned_text'] = test_data['text'].apply(preprocess_text)\n","\n","# Display preprocessed examples\n","# print(train_data[['text', 'cleaned_text']].head())"]},{"cell_type":"markdown","metadata":{"id":"1kfX0KewquUV"},"source":["As the dataset is cleaned now, the AutoTokenizer is designated to convert the text in a form the TimeLMLARGE understands"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GRZttH2Li1ti"},"outputs":[],"source":["# Initialize tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-large-2022-154m\")\n","\n","# Tokenization function\n","def tokenize_data(df, text_col, label_col, max_length=128):\n","    tokens = tokenizer(\n","        df[text_col].tolist(),\n","        max_length=max_length,\n","        padding=\"max_length\",\n","        truncation=True,\n","        return_tensors=\"pt\"\n","    )\n","    labels = df[label_col].tolist()\n","    return tokens, labels\n","\n","# Tokenize datasets\n","train_tokens, train_labels = tokenize_data(train_data, \"cleaned_text\", \"gold_label\")\n","validation_tokens, validation_labels = tokenize_data(validation_data, \"cleaned_text\", \"gold_label\")\n","test_tokens, test_labels = tokenize_data(test_data, \"cleaned_text\", \"gold_label\")"]},{"cell_type":"markdown","metadata":{"id":"TUUA_8iCrP5k"},"source":["As the task is complicated, the batch size for the training set is relatively small. This helps avoiding aggressive learning and digesting the linguistic nuances of the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3bYzGjobi62M"},"outputs":[],"source":["# Create TensorDatasets\n","train_dataset = TensorDataset(train_tokens['input_ids'], train_tokens['attention_mask'], torch.tensor(train_labels))\n","validation_dataset = TensorDataset(validation_tokens['input_ids'], validation_tokens['attention_mask'], torch.tensor(validation_labels))\n","test_dataset = TensorDataset(test_tokens['input_ids'], test_tokens['attention_mask'], torch.tensor(test_labels))\n","\n","# Create DataLoaders\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","validation_loader = DataLoader(validation_dataset, batch_size=64, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"T9E4zvCerqX6"},"source":["In this code block, the model start learning with the following hyperparamters:\n","1. 15 epochs as the task a relative number of emojis becuase the number of emojis are 100\n","2. 5e-6 learning rate\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GU_1PvPujC2d"},"outputs":[],"source":["# Load the model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-large-2022-154m\", num_labels = 100)\n","model.to(device)\n","\n","# Compute class weights for imbalanced data\n","class_counts = train_data['gold_label'].value_counts()\n","class_weights = torch.tensor(1.0 / class_counts).to(device)\n","\n","# Define optimizer and scheduler\n","optimizer = AdamW(model.parameters(), lr=5e-6)\n","num_training_steps = len(train_loader) * 15  # For 15 epochs\n","scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","\n","# Training loop\n","epochs = 15\n","for epoch in range(epochs):\n","    model.train()\n","    total_loss = 0\n","\n","    for batch in train_loader:\n","        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        total_loss += loss.item()\n","\n","        # Backward pass\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","    print(f\"Epoch {epoch+1}/{epochs} - Training Loss: {total_loss / len(train_loader)}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C35mOIvUrM0R"},"outputs":[],"source":["# Function to output predictions to save to file, and evaluate Top-1 and Top-5 accuracy\n","def evaluate(model, loader):\n","    model.eval()\n","    total, correct_top1, correct_top5 = 0, 0, 0\n","    all_top1_preds, all_top5_preds, all_labels = [], [], []\n","\n","    with torch.no_grad():\n","        for e, batch in enumerate(loader):\n","            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n","            outputs = model(input_ids, attention_mask=attention_mask)\n","            logits = outputs.logits\n","            probs = F.softmax(logits, dim=1)\n","\n","            # Top-1 accuracy\n","            top1_preds = torch.argmax(probs, dim=1)\n","            correct_top1 += (top1_preds == labels).sum().item()\n","\n","            # Top-5 accuracy\n","            top5_preds = torch.topk(probs, k=5, dim=1).indices\n","            correct_top5 += torch.sum(torch.any(top5_preds == labels.unsqueeze(1), dim=1)).item()\n","\n","            # Collect all predictions together\n","            all_top1_preds.extend(top1_preds.cpu().numpy())\n","            all_top5_preds.extend(top5_preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","            total += labels.size(0)\n","\n","    top1_accuracy = correct_top1 / total\n","    top5_accuracy = correct_top5 / total\n","    return top1_accuracy, top5_accuracy, all_top1_preds, all_top5_preds, all_labels\n","\n","# Evaluate on the separate test dataset\n","test_top1, test_top5, all_t1, all_t5, all_lbls = evaluate(model, test_loader)\n","print(f\"Test Top-1 Accuracy: {test_top1:.4f}, Test Top-5 Accuracy: {test_top5:.4f}\")\n","\n","# for top1, top5, label in list(zip(t1, t5, lbls))[:10]:\n","#   print(f\"Top1: {top1}, Top5: {top5}, Label: {label}\")\n","\n","# Save all predictions to csv file\n","df_results = pd.DataFrame(columns=['top1', 'top5', 'label'], data=list(zip(all_t1, all_t5, all_lbls)))\n","df_results.to_csv('results_timelm.csv')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"python_venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.8"}},"nbformat":4,"nbformat_minor":0}