{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PCsd9yk5kML"
   },
   "source": [
    "# SVM Model\n",
    "An SVM model for emoji prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HosXMPPO5ekw",
    "outputId": "febf99ee-5ed9-4576-b200-88ed38932339"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gensim\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import scipy\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "\n",
    "from pandas import DataFrame\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MSUVCuXP6cow"
   },
   "outputs": [],
   "source": [
    "# Load data from files\n",
    "# Dataframes have columns 'text', 'gold_label' and 'date'\n",
    "\n",
    "dirpath = ''\n",
    "\n",
    "df_train = pd.read_csv(dirpath + 'emoji-train.csv')\n",
    "df_test = pd.read_csv(dirpath + 'emoji-test.csv')\n",
    "df_val = pd.read_csv(dirpath + 'emoji-validation.csv')\n",
    "\n",
    "train_X = df_train['text']\n",
    "train_y = df_train['gold_label']\n",
    "train_date = df_train['date']\n",
    "\n",
    "val_X = df_val['text']\n",
    "val_gold_y = df_val['gold_label']\n",
    "val_date = df_val['date']\n",
    "\n",
    "test_X = df_test['text']\n",
    "test_gold_y = df_test['gold_label']\n",
    "test_date = df_test['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Y_ENopUFiYbj"
   },
   "outputs": [],
   "source": [
    "# Use the pre-trained google news w2vec dataset\n",
    "import gensim.downloader as api\n",
    "\n",
    "defaultLoad = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysEBzrTCiYbj"
   },
   "source": [
    "## Vectorizer classes ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2Po6MkYAiYbk"
   },
   "outputs": [],
   "source": [
    "# Word2Vectorizer class\n",
    "# The class mimics TfidfVectorizer's structure, so that it can be used\n",
    "# as a generic vectorizer in the pipeline\n",
    "class Word2Vectorizer():\n",
    "    # Constructor\n",
    "    # Parameters: w2vec model (optional, default is w2vec trained on google news dataset), number of features, w2vec_url (optional)\n",
    "    def __init__(self, w2vec = defaultLoad, w2vec_NUM_FEATURES = 300, w2vec_url = None):\n",
    "        if w2vec_url == None:\n",
    "            self.w2vec = w2vec\n",
    "        else:\n",
    "            self.w2vec = api.load(w2vec_url)\n",
    "        self.w2vec_NUM_FEATURES = w2vec_NUM_FEATURES\n",
    "        return\n",
    "    # No need to fit since only pretrained w2vec models are used\n",
    "    def fit(self, X, y = None):\n",
    "        return\n",
    "    # transform function\n",
    "    # Parameters: Document\n",
    "    # Given a document, obtain a vector by getting the mean of all of its words' w2vec vectors\n",
    "    def transform(self, X, y = None):\n",
    "        ans = []\n",
    "        for document in X:\n",
    "            avg = np.zeros(self.w2vec_NUM_FEATURES)\n",
    "            num_nonzero = 0\n",
    "            for word in document:\n",
    "                word_cpy = word.lower()\n",
    "                if word_cpy in self.w2vec.key_to_index:\n",
    "                    avg = (avg + self.w2vec[word_cpy])\n",
    "                    num_nonzero += 1\n",
    "            ans.append(np.asarray(avg / num_nonzero))\n",
    "        return np.asarray(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "G0adJsU8iYbk"
   },
   "outputs": [],
   "source": [
    "# Class CharacterTfidfVectorizer\n",
    "# This implements the character tfidf vectorizer for a document\n",
    "# This is done by introducing spaces between each character in the document,\n",
    "# So that each character is separated\n",
    "# The structure is similar to Word2Vectorizer\n",
    "class CharacterBagofngramsVectorizer():\n",
    "    # Constructor\n",
    "    # Parameters: ngram_range (tuple of two numbers)\n",
    "    def __init__(self, ngram_range):\n",
    "        # Initialize a Tfidf vectorizer\n",
    "        self.vectoriser = CountVectorizer(lowercase = False, ngram_range = ngram_range, stop_words = None, min_df = 1, vocabulary = [chr(ch) for ch in range(128)], max_features = 2500)\n",
    "    # Helper function for introducing spaces between characters\n",
    "    # Parameters: corpus X\n",
    "    def space_doc(self, X):\n",
    "        X_copy = X\n",
    "        X_to_fit = []\n",
    "        for index in range(len(X_copy)):\n",
    "            doc = \"\"\n",
    "            for index_2 in range(len(X_copy[index])):\n",
    "                doc = doc + X_copy[index][index_2] + ' '\n",
    "            doc = doc[:-1]\n",
    "            X_to_fit.append(doc)\n",
    "        # print(X_to_fit[:2])\n",
    "        return X_to_fit\n",
    "    # Function fit that trains the tfidf vectorizer\n",
    "    def fit(self, X, y = None):\n",
    "        self.vectoriser.fit(self.space_doc(X))\n",
    "    # Function transform that transforms the corpus into a set of vectors\n",
    "    def transform(self, X, y = None):\n",
    "        return self.vectoriser.transform(self.space_doc(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UvcTcy81iYbk"
   },
   "outputs": [],
   "source": [
    "# Helper functions to construct vectorizers / classifiers\n",
    "def getTfidfVectorizer(ngram_range_param):\n",
    "    # max_features was used because of a notable jump in accuracy when using it\n",
    "    return TfidfVectorizer(lowercase = True, stop_words = stopwords.words('english'), ngram_range = ngram_range_param, min_df = 1, max_features = 2500)\n",
    "\n",
    "\n",
    "def getW2vecVectorizer():\n",
    "    return Word2Vectorizer()\n",
    "\n",
    "\n",
    "def getBagOfngramsVectorizer(ngram_range_param):\n",
    "    return CountVectorizer(lowercase = True, ngram_range = ngram_range_param, min_df = 1, max_features = 2500)\n",
    "\n",
    "\n",
    "def getCharacterBagOfngramsVectorizer(char_ngram_range_param):\n",
    "    return CharacterBagofngramsVectorizer(ngram_range = char_ngram_range_param)\n",
    "\n",
    "\n",
    "def getSGDC(C = 0.1, max_iter = None):\n",
    "    if max_iter != None:\n",
    "        return SGDClassifier(alpha = len(train_y) / C, max_iter = max_iter, n_jobs = -1, loss = 'squared_hinge', n_iter_no_change = 100)\n",
    "    else:\n",
    "        return SGDClassifier(alpha = len(train_y) / C, n_iter_no_change = 100, loss = 'squared_hinge', n_jobs = -1)\n",
    "\n",
    "def getLinearSVMC(C = 0.1, max_iter = None):\n",
    "    if max_iter != None:\n",
    "        return OneVsRestClassifier(LinearSVC(C = C, max_iter = max_iter), n_jobs = -1)\n",
    "    else:\n",
    "        return OneVsRestClassifier(LinearSVC(C = C), n_jobs = -1)\n",
    "\n",
    "def getPolySVMC(C = 0.1, max_iter = None, degree = 3, gamma = 'scale', coef0 = 0):\n",
    "    if max_iter != None:\n",
    "        return OneVsRestClassifier(BaggingClassifier(SVC(kernel = 'poly', max_iter = max_iter, C = C, degree = degree, gamma = gamma, coef0 = coef0, class_weight='balanced'), n_estimators = 10, max_samples = 1 / 10), n_jobs = -1)\n",
    "    else:\n",
    "        return OneVsRestClassifier(BaggingClassifier(SVC(kernel = 'poly', C = C, degree = degree, gamma = gamma, coef0 = coef0, class_weight='balanced'), n_estimators = 10, max_samples = 1 / 10), n_jobs = -1)\n",
    "\n",
    "def getRbfSVMC(C = 0.1, max_iter = None, gamma = 'scale'):\n",
    "    if max_iter != None:\n",
    "        return OneVsRestClassifier(BaggingClassifier(SVC(kernel = 'rbf', max_iter = max_iter, C = C, gamma = gamma, class_weight='balanced'), n_estimators = 10, max_samples = 1 / 10), n_jobs = -1)\n",
    "    else:\n",
    "        return OneVsRestClassifier(BaggingClassifier(SVC(kernel = 'rbf', C = C, gamma = gamma, class_weight='balanced'), n_estimators = 10, max_samples = 1 / 10), n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hour to Time of day map #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_time = {}\n",
    "# Early morning\n",
    "for index in range(0, 5):\n",
    "    category_time[index] = 0\n",
    "# Morning\n",
    "for index in range(5, 11):\n",
    "    category_time[index] = 1\n",
    "# Noon\n",
    "for index in range(11, 15):\n",
    "    category_time[index] = 2\n",
    "# Afternoon\n",
    "for index in range(15, 18):\n",
    "    category_time[index] = 3\n",
    "# Evening\n",
    "for index in range(18, 21):\n",
    "    category_time[index] = 4\n",
    "# Night\n",
    "for index in range(21, 24):\n",
    "    category_time[index] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7MCPjm5iYbl"
   },
   "source": [
    "## Pipeline class ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to limitations of the sklearn pipeline with custom vectorizers as well as having a separate date set, a custom pipeline class was created. It takes vectorisers, whether or not to use feature selection, and a classifier as its arguments.\n",
    "To fit the pipeline, use the fit function with the corpus, date set and label set.\n",
    "To get the prediction of the pipeline, either use the predict or the predict_proba functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ny3FTGjk6Vsz"
   },
   "outputs": [],
   "source": [
    "# Whether to have the pipelines print internal messages or not\n",
    "VERBOSITY = False\n",
    "\n",
    "# Pipeline class\n",
    "# Mimics the sklearn pipeline, but allows for less constrained estimators to be used\n",
    "class myPipeline:\n",
    "    # Constructor: takes in a list of vectorisers, whether feature selection is used and how many features should be used,\n",
    "    # and the classifier to be used\n",
    "    # Parameters: list vectorisers, string feature_selection, integer k_selectKBest, classifier\n",
    "    def __init__(self, *vectorisers, feature_selection = None, k_selectKBest = 2000, classifier):\n",
    "        # Create the internal vectoriser list\n",
    "        self.vectorisers = []\n",
    "        for vectoriser in vectorisers:\n",
    "            self.vectorisers.append(vectoriser)\n",
    "        # Create the internal classifier\n",
    "        self.classifier = classifier\n",
    "        # Flag to keep track of whether feature selection has been trained\n",
    "        self.has_trained_feature_selection = False\n",
    "        # Set the correct feature selection\n",
    "        if feature_selection == None:\n",
    "            self.feature_selection = None\n",
    "            self.feature_selection_type = None\n",
    "        elif feature_selection == \"SelectKBest\":\n",
    "            self.feature_selection = SelectKBest(k = k_selectKBest)\n",
    "            self.feature_selection_type = \"SelectKBest\"\n",
    "        else:\n",
    "            raise Exception(\"Invalid feature selection\")\n",
    "        self.sc = MaxAbsScaler()\n",
    "    # Date preprocessing\n",
    "    def extract_date_info(self, dates):\n",
    "        ans = []\n",
    "        for date in dates:\n",
    "            ans.append(np.asarray([category_time[(int)(date[11] + date[12])], (int)(date[0 : 4]), (int)(date[5 : 7]), (int)(date[8 : 10])]))\n",
    "        return np.asarray(ans)\n",
    "    # Internal function to preprocess the corpus\n",
    "    def preprocess(self, corpus, dates):\n",
    "        # Create a list of the corpus transformed by each vectoriser\n",
    "        raw = []\n",
    "        raw.append(self.extract_date_info(dates))\n",
    "        for index in range(len(self.vectorisers)):\n",
    "            raw.append(self.vectorisers[index].transform(corpus))\n",
    "        # If feature selection has not been trained, then don't do feature selection on the corpus\n",
    "        if self.has_trained_feature_selection == False:\n",
    "            try:\n",
    "                return scipy.sparse.hstack(raw)\n",
    "            except:\n",
    "                # this happens when only w2vec is used\n",
    "                return raw[0]\n",
    "        else:\n",
    "            # Otherwise, do feature selection\n",
    "            try:\n",
    "                return self.feature_selection.transform(scipy.sparse.hstack(raw))\n",
    "            except:\n",
    "                # this happens when only w2vec is used\n",
    "                return self.feature_selection.transform(raw[0])\n",
    "    def my_softmax(self, val_lst):\n",
    "        ans = []\n",
    "        sum = 0.0\n",
    "        for val in val_lst:\n",
    "            ans.append(math.e ** val)\n",
    "            sum = sum + math.e ** val\n",
    "        for index in range(len(ans)):\n",
    "            ans[index] /= sum\n",
    "        return np.asarray(ans)\n",
    "\n",
    "    def my_predict_proba(self, test_X):\n",
    "        proba_dist = self.classifier.decision_function(test_X)\n",
    "        proba_list = []\n",
    "        for instance in proba_dist:\n",
    "            proba_list.append(self.my_softmax(instance))\n",
    "        return np.asarray(proba_list)\n",
    "    # fit function, requires both a corpus and a list of labels\n",
    "    def fit(self, train_X, train_date, train_y):\n",
    "        if VERBOSITY:\n",
    "            print(\"Fitting started\")\n",
    "\n",
    "        # Fit every vectorizer\n",
    "        for index in range(len(self.vectorisers)):\n",
    "            self.vectorisers[index].fit(train_X)\n",
    "\n",
    "        # Do an initial vectorization of the corpus, so that feature selection can be trained\n",
    "        preprocessed_X = self.preprocess(train_X, train_date)\n",
    "        # Train the feature selector\n",
    "        if VERBOSITY:\n",
    "            print(\"Pre-processed training set shape (before feature selection):\")\n",
    "            print(preprocessed_X.shape)\n",
    "        if self.feature_selection_type == \"SelectKBest\":\n",
    "            self.feature_selection.fit(preprocessed_X, train_y)\n",
    "            self.has_trained_feature_selection = True\n",
    "            preprocessed_X = self.feature_selection.transform(preprocessed_X)\n",
    "        preprocessed_X = self.sc.fit_transform(preprocessed_X)\n",
    "        if VERBOSITY:\n",
    "            print(\"Pre-processed training set shape:\")\n",
    "            print(preprocessed_X.shape)\n",
    "        # Train the classifier\n",
    "        self.classifier.fit(preprocessed_X, train_y)\n",
    "    # given a test set, predict its labels\n",
    "    def predict(self, test_X, test_date):\n",
    "        return self.classifier.predict(self.sc.transform(self.preprocess(test_X, test_date)))\n",
    "    # given a test set, predict the label probabilities\n",
    "    def predict_proba(self, test_X, test_date):\n",
    "        return self.my_predict_proba(self.sc.transform(self.preprocess(test_X, test_date)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFUT_rpEiYbl"
   },
   "source": [
    "## Simulated Annealing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kh7e-76XiYbl"
   },
   "outputs": [],
   "source": [
    "# Defining a state:\n",
    "# (C, n-gram size, character n-gram size, feature_engineering_combination, selectkbest_k, max_iter)\n",
    "\n",
    "class State:\n",
    "    # Constructor\n",
    "    def __init__(self, model = getLinearSVMC):\n",
    "        # You can change getLinearSVMC to other model functions for different kernels (poly / rbf)\n",
    "        # getPolySVMC or getRbfSVMC\n",
    "        self.model = model\n",
    "        self.C_range = [0.02 + x * 0.02 for x in range((int)(1.0 / 0.02))]\n",
    "        self.n_gram_range = [(1, 1), (1, 2), (1, 3), (1, 4)]\n",
    "        self.char_n_gram_range = [(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6)]\n",
    "        self.feature_engineering_possibilities = [\"w2vec\", \"bag_of_char_ngrams\", \"bag_of_ngrams\", \"tfidf\"]\n",
    "        #                                           0           1                   2               3\n",
    "        #    12 = 8 + 4 = 2 ** 3 + 2 ** 2\n",
    "        #\n",
    "        self.selectkbest_k_possibilities = [250, 500, 750, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000]\n",
    "        self.max_iter_possibilities = [(x + 1) * 1000 for x in range(50)]\n",
    "        self.score = None\n",
    "        self.has_trained_model = False\n",
    "        self.pipe = None\n",
    "        (self.C_idx, self.n_gram_idx, self.char_n_gram_idx, self.feature_engineering_combination_idxs, self.selectkbest_k_idx, self.max_iter_idx) = self.get_random()\n",
    "\n",
    "        self.vec_dict = {\n",
    "            \"w2vec\" : getW2vecVectorizer,\n",
    "            \"bag_of_char_ngrams\" : getCharacterBagOfngramsVectorizer,\n",
    "            \"bag_of_ngrams\" : getBagOfngramsVectorizer,\n",
    "            \"tfidf\" : getTfidfVectorizer\n",
    "        }\n",
    "\n",
    "        self.vec_params = {\n",
    "            \"w2vec\" : {},\n",
    "            \"bag_of_char_ngrams\" : {\n",
    "                \"char_ngram_range_param\" : \"self.char_n_gram_range[self.char_n_gram_idx]\"\n",
    "            },\n",
    "            \"bag_of_ngrams\" : {\n",
    "                \"ngram_range_param\" : \"self.n_gram_range[self.n_gram_idx]\"\n",
    "            },\n",
    "            \"tfidf\" : {\n",
    "                \"ngram_range_param\" : \"self.n_gram_range[self.n_gram_idx]\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "    # Returns a random combination of parameters\n",
    "    def get_random(self):\n",
    "        C_idx = random.choice(range(len(self.C_range)))\n",
    "        n_gram_idx = random.choice(range(len(self.n_gram_range)))\n",
    "        char_n_gram_idx = random.choice(range(len(self.char_n_gram_range)))\n",
    "        feature_engineering_combination_idxs = 1 + random.choice(range(2 ** (len(self.feature_engineering_possibilities)) - 1))\n",
    "        # range(15) = [0..14] + 1 => [1..15]\n",
    "        selectkbest_k_idx = random.choice(range(len(self.selectkbest_k_possibilities)))\n",
    "        max_iter_idx = random.choice(range(len(self.max_iter_possibilities)))\n",
    "        return (C_idx, n_gram_idx, char_n_gram_idx, feature_engineering_combination_idxs, selectkbest_k_idx, max_iter_idx)\n",
    "\n",
    "    # Returns a random feature engineering neighbour\n",
    "    def get_feature_eng_comb_neighbour(self):\n",
    "        possibilities = [self.feature_engineering_combination_idxs]\n",
    "        for idx in range(len(self.feature_engineering_possibilities)):\n",
    "            feature_eng_comb_neigh = self.feature_engineering_combination_idxs ^ (2 ** idx)\n",
    "            if(feature_eng_comb_neigh != 0):\n",
    "                possibilities.append(feature_eng_comb_neigh)\n",
    "        return random.choice(possibilities)\n",
    "\n",
    "    # Returns a state neighbour\n",
    "    # Let's say our current state is C = 0.04, n_gram = (1, 3), char_n_gram = (1, 2), feature_eng = [w2vec], selectkbest_k = 1250, max_iter = 10000\n",
    "    # You could have a neighbour with C = 0.04, n_gram = (1, 3), char_n_gram = (1, 2), feature_eng = [w2vec], selectkbest_k = 1250, max_iter = 9000\n",
    "    def get_neighbour(self):\n",
    "        dx_vec = [-1, 0, 1]\n",
    "        C_idx = random.choice(dx_vec) + self.C_idx\n",
    "        n_gram_idx = random.choice(dx_vec) + self.n_gram_idx\n",
    "        char_n_gram_idx = random.choice(dx_vec) + self.char_n_gram_idx\n",
    "        feature_engineering_combination_idxs = self.get_feature_eng_comb_neighbour()\n",
    "        selectkbest_k_idx = random.choice(dx_vec) + self.selectkbest_k_idx\n",
    "        max_iter_idx = random.choice(dx_vec) + self.max_iter_idx\n",
    "        if((C_idx, n_gram_idx, char_n_gram_idx, feature_engineering_combination_idxs, selectkbest_k_idx, max_iter_idx) == (self.C_idx, self.n_gram_idx, self.char_n_gram_idx, self.feature_engineering_combination_idxs, self.selectkbest_k_idx, self.max_iter_idx)):\n",
    "            return self.get_neighbour()\n",
    "        neighbour = State()\n",
    "        neighbour.C_idx = C_idx % len(self.C_range)\n",
    "        neighbour.n_gram_idx = n_gram_idx % len(self.n_gram_range)\n",
    "        neighbour.char_n_gram_idx = char_n_gram_idx % len(self.char_n_gram_range)\n",
    "        neighbour.feature_engineering_combination_idxs = feature_engineering_combination_idxs\n",
    "        neighbour.selectkbest_k_idx = selectkbest_k_idx % len(self.selectkbest_k_possibilities)\n",
    "        neighbour.max_iter_idx = max_iter_idx % len(self.max_iter_possibilities)\n",
    "        return neighbour\n",
    "\n",
    "    # Helper function to compile a list of vectorizers from the feature_engineering_possibilities number\n",
    "    def get_vectorizers(self):\n",
    "        vectorizers = []\n",
    "\n",
    "        for idx in range(len(self.feature_engineering_possibilities)):\n",
    "            if((2 ** idx) & self.feature_engineering_combination_idxs) != 0:\n",
    "                vec_name = self.feature_engineering_possibilities[idx]\n",
    "                vectorizers.append((self.vec_dict[vec_name], self.vec_params[vec_name]))\n",
    "\n",
    "        return vectorizers\n",
    "\n",
    "    # Turns a binary representation into corresponding feature engineering methods\n",
    "    def get_str_feature_eng_comb_idxs(self):\n",
    "        ans = \"\"\n",
    "        for index in range(len(self.feature_engineering_possibilities)):\n",
    "            if((2 ** index) & self.feature_engineering_combination_idxs):\n",
    "                ans += self.feature_engineering_possibilities[index] + \"; \"\n",
    "        return ans\n",
    "\n",
    "    # Returns a string detailing the state's hyperparameters\n",
    "    def str_state(self):\n",
    "        ans = \"\"\n",
    "        ans += \"C parameter: \" + str(self.C_range[self.C_idx]) + \"\\n\"\n",
    "        ans += \"n-gram parameter: \" + str(self.n_gram_range[self.n_gram_idx]) + \"\\n\"\n",
    "        ans += \"char n-gram parameter: \" + str(self.char_n_gram_range[self.char_n_gram_idx]) + \"\\n\"\n",
    "        ans += \"feature engineering used: \" + self.get_str_feature_eng_comb_idxs() + \"date pre-processing\\n\"\n",
    "        ans += \"selectkbest k parameter: \" + str(self.selectkbest_k_possibilities[self.selectkbest_k_idx]) + \"\\n\"\n",
    "        ans += \"max_iter parameter: \" + str(self.max_iter_possibilities[self.max_iter_idx]) + \"\\n\"\n",
    "        return ans\n",
    "\n",
    "    # Helper function to print the state's hyperparameters\n",
    "    def print_state(self):\n",
    "        print(self.str_state())\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    # Helper function to print the state's hyperparameters to a file\n",
    "    def print_state_file(self, file):\n",
    "        file.write(self.str_state() + \"\\n\")\n",
    "\n",
    "    # Helper function that returns a dictionary of parameter names with their values\n",
    "    def process_params(self, params):\n",
    "        processed_params = {}\n",
    "        for param_name, param_val_name in params.items():\n",
    "            processed_params[param_name] = eval(param_val_name)\n",
    "        return processed_params\n",
    "\n",
    "    # Evaluates the state's performance, taking the energy to be negative accuracy score\n",
    "    def get_energy(self):\n",
    "        if self.score != None:\n",
    "            return self.score\n",
    "        vectorizer_list = self.get_vectorizers()\n",
    "        applied_vec = []\n",
    "        for (func, params) in vectorizer_list:\n",
    "            applied_vec.append(func(**self.process_params(params)))\n",
    "\n",
    "        if self.has_trained_model == False:\n",
    "            self.pipe = myPipeline(*applied_vec, classifier = self.model(C = self.C_range[self.C_idx], max_iter = self.max_iter_possibilities[self.max_iter_idx]), feature_selection = \"SelectKBest\", k_selectKBest = self.selectkbest_k_possibilities[self.selectkbest_k_idx])\n",
    "            self.has_trained_model = True\n",
    "            self.pipe.fit(train_X, train_date, train_y)\n",
    "\n",
    "        val_pred_y_probs = self.pipe.predict_proba(val_X, val_date)\n",
    "        pred_ids = np.argpartition(val_pred_y_probs, kth=-5, axis=1)\n",
    "        candidate_preds = pred_ids[:, -5:]\n",
    "\n",
    "        val_pred_y = [gold if gold in candidate_preds[e] else candidate_preds[e, -1] for e, gold in enumerate(val_gold_y)]\n",
    "        self.score = -(accuracy_score(val_gold_y, val_pred_y))\n",
    "        return self.score\n",
    "    # Evaluates a topk accuracy measure on the validation set\n",
    "    def model_top_k_validation(self, k = 5):\n",
    "        vectorizer_list = self.get_vectorizers()\n",
    "        applied_vec = []\n",
    "        for (func, params) in vectorizer_list:\n",
    "            applied_vec.append(func(**self.process_params(params)))\n",
    "\n",
    "        # Make sure the pipeline has been trained\n",
    "        if self.has_trained_model == False:\n",
    "            self.pipe = myPipeline(*applied_vec, classifier = self.model(C = self.C_range[self.C_idx], max_iter = self.max_iter_possibilities[self.max_iter_idx]), feature_selection = \"SelectKBest\", k_selectKBest = self.selectkbest_k_possibilities[self.selectkbest_k_idx])\n",
    "            self.has_trained_model = True\n",
    "            self.pipe.fit(train_X, train_date, train_y)\n",
    "\n",
    "        val_pred_y_probs = self.pipe.predict_proba(val_X, val_date)\n",
    "        pred_ids = np.argpartition(val_pred_y_probs, kth=-k, axis=1)\n",
    "        candidate_preds = pred_ids[:, -k:]\n",
    "\n",
    "        val_pred_y = [gold if gold in candidate_preds[e] else candidate_preds[e, -1] for e, gold in enumerate(val_gold_y)]\n",
    "        return (accuracy_score(val_gold_y, val_pred_y))\n",
    "\n",
    "    # Evaluates a topk accuracy measure on the test set\n",
    "    def eval_model_top_k(self, k = 5):\n",
    "        vectorizer_list = self.get_vectorizers()\n",
    "        applied_vec = []\n",
    "        for (func, params) in vectorizer_list:\n",
    "            applied_vec.append(func(**self.process_params(params)))\n",
    "        \n",
    "        if self.has_trained_model == False:\n",
    "            self.pipe = myPipeline(*applied_vec, classifier = self.model(C = self.C_range[self.C_idx], max_iter = self.max_iter_possibilities[self.max_iter_idx]), feature_selection = \"SelectKBest\", k_selectKBest = self.selectkbest_k_possibilities[self.selectkbest_k_idx])\n",
    "            self.has_trained_model = True\n",
    "            self.pipe.fit(train_X, train_date, train_y)\n",
    "\n",
    "        test_pred_y_probs = self.pipe.predict_proba(test_X, test_date)\n",
    "        pred_ids = np.argpartition(test_pred_y_probs, kth=-k, axis=1)\n",
    "        candidate_preds = pred_ids[:, -k:]\n",
    "\n",
    "        test_pred_y = [gold if gold in candidate_preds[e] else candidate_preds[e, -1] for e, gold in enumerate(test_gold_y)]\n",
    "        return accuracy_score(test_gold_y, test_pred_y)\n",
    "\n",
    "# Simulated annealing's acceptance function\n",
    "def acceptance_function(energy_state, energy_neighbour, temperature):\n",
    "    if(energy_state > energy_neighbour):\n",
    "        return 1\n",
    "    if(temperature < 0.0001):\n",
    "        return 0\n",
    "    return math.e ** (100 / 30 * 1.40670535838 * 1.15 * -(energy_neighbour - energy_state) / temperature)\n",
    "\n",
    "# Prints the top5 and top1 accuracy of a state, on the validation set\n",
    "def print_validation_stats(state):\n",
    "    print(\"[Validation] Accuracy top 5: \" + str(state.model_top_k_validation(k = 5)) + \" ; Accuracy top 1: \" + str(state.model_top_k_validation(k = 1)))\n",
    "\n",
    "# Prints the top5 and top1 accuracy of a state, on the validation set, to file\n",
    "def print_validation_stats_file(state, file):\n",
    "    file.write(\"[Validation] Accuracy top 5: \" + str(state.model_top_k_validation(k = 5)) + \" ; Accuracy top 1: \" + str(state.model_top_k_validation(k = 1)) + '\\n')\n",
    "\n",
    "# Prints the top5 and top1 accuracy of a state, on the test set\n",
    "def print_evaluation_stats(state):\n",
    "    print(\"[Evaluation] Accuracy top 5: \" + str(state.eval_model_top_k(k = 5)) + \" ; Accuracy top 1: \" + str(state.eval_model_top_k(k = 1)))\n",
    "\n",
    "# Prints the top5 and top1 accuracy of a state, on the test set, to file\n",
    "def print_evaluation_stats_file(state, file):\n",
    "    file.write(\"[Evaluation] Accuracy top 5: \" + str(state.eval_model_top_k(k = 5)) + \" ; Accuracy top 1: \" + str(state.eval_model_top_k(k = 1)) + '\\n')\n",
    "\n",
    "# Helper function to copy state from src to dest\n",
    "def cpy_state(dest, src):\n",
    "    dest.score = src.score\n",
    "    dest.has_trained_model = src.has_trained_model\n",
    "    dest.pipe = src.pipe\n",
    "    (dest.C_idx, dest.n_gram_idx, dest.char_n_gram_idx, dest.feature_engineering_combination_idxs, dest.selectkbest_k_idx, dest.max_iter_idx) = (src.C_idx, src.n_gram_idx, src.char_n_gram_idx, src.feature_engineering_combination_idxs, src.selectkbest_k_idx, src.max_iter_idx)\n",
    "\n",
    "# The simulated annealing algorithm\n",
    "def simulated_annealing(no_steps = 100, verbosity = True, file = None):\n",
    "\n",
    "    # Start with random state\n",
    "    state = State()\n",
    "\n",
    "    # Initialize the best overall state to the random state\n",
    "    best_overall = State()\n",
    "    cpy_state(best_overall, state)\n",
    "\n",
    "    # Print starting state\n",
    "    if verbosity:\n",
    "        print(\"Starting state:\")\n",
    "        state.print_state()\n",
    "        print_validation_stats(state)\n",
    "    if file != None:\n",
    "        file.write(\"Starting state:\\n\")\n",
    "        state.print_state_file(file)\n",
    "        print_validation_stats_file(state, file)\n",
    "        \n",
    "    # Run the algorithm for no_steps steps\n",
    "    for k in range(no_steps):\n",
    "        if verbosity:\n",
    "            print(\"Step number # \" + str(k + 1) + \" out of \" + str(no_steps))\n",
    "        # Get temperature\n",
    "        temperature = 1 - (k + 1) / no_steps\n",
    "        # Get random neighbour\n",
    "        state_neigh = state.get_neighbour()\n",
    "        if verbosity:\n",
    "            state_neigh.print_state()\n",
    "            print_validation_stats(state_neigh)\n",
    "        if file != None:\n",
    "            file.write(\"Step number # \" + str(k + 1) + \" out of \" + str(no_steps) + '\\n')\n",
    "            state_neigh.print_state_file(file)\n",
    "            print_validation_stats_file(state_neigh, file)\n",
    "\n",
    "        # Keep the best_overall updated to have the highest validation set accuracy\n",
    "        if best_overall.get_energy() > state_neigh.get_energy():\n",
    "            cpy_state(best_overall, state_neigh)\n",
    "\n",
    "        # Use the acceptance function do determine if a move to the neighbour is made\n",
    "        # If the energy of the neighbour is lower, then always move to the neighbour\n",
    "        if acceptance_function(state.get_energy(), state_neigh.get_energy(), temperature) >= random.uniform(0, 1):\n",
    "            state.score = state_neigh.score \n",
    "            cpy_state(state, state_neigh)\n",
    "            if verbosity:\n",
    "                print(\"Admitted state\")\n",
    "            if file != None:\n",
    "                file.write(\"Admitted state\\n\")\n",
    "    if verbosity:\n",
    "        print(\"Best model:\")\n",
    "        best_overall.print_state()\n",
    "        print_evaluation_stats(best_overall)\n",
    "    if file != None:\n",
    "        file.write(\"Best model:\\n\")\n",
    "        best_overall.print_state_file(file)\n",
    "        print_evaluation_stats_file(best_overall, file)\n",
    "    return best_overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the different kernels ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# This section was commented because of performance issues with Rbf and Poly kernels\n",
    "'''\n",
    "C_idx_comparison = 30\n",
    "n_gram_idx_comparison = 2\n",
    "char_n_gram_idx_comparison = 3\n",
    "feature_engineering_combination_idxs_comparison = 10\n",
    "selectkbest_k_idx_comparison = 3\n",
    "max_iter_idx_comparison = 10\n",
    "for (model, model_name) in [(getLinearSVMC, 'linear kernel'), (getPolySVMC, 'poly kernel'), (getRbfSVMC, 'rbf kernel')]:\n",
    "    print(\"Model stats for \" + model_name + \":\")\n",
    "    state = State(model = model)\n",
    "    state.C_idx = C_idx_comparison\n",
    "    state.n_gram_idx = n_gram_idx_comparison\n",
    "    state.char_n_gram_idx = char_n_gram_idx_comparison\n",
    "    state.feature_engineering_combination_idxs = feature_engineering_combination_idxs_comparison\n",
    "    state.selectkbest_k_idx = selectkbest_k_idx_comparison\n",
    "    state.max_iter_idx = max_iter_idx_comparison\n",
    "    state.print_state()\n",
    "    print(\"Accuracy top-1: \" + str(state.eval_model_top_k(k = 1)))\n",
    "    print(\"Accuracy top-5: \" + str(state.eval_model_top_k(k = 5)))\n",
    "'''\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KIBKvH8biYbm",
    "outputId": "1e4b1b54-a1fc-4c16-873a-43dd3e0687e2"
   },
   "outputs": [],
   "source": [
    "# Run simulated annealing with 200 steps\n",
    "best_state = simulated_annealing(no_steps = 200, file = open(\"results_200_1.txt\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the structure of the best state\n",
    "best_state.print_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the evaluation of a model and put it into a csv file\n",
    "def generate_csv(state):\n",
    "    k = 5\n",
    "    vectorizer_list = state.get_vectorizers()\n",
    "    applied_vec = []\n",
    "    # Gathering vectoriser arguments\n",
    "    \n",
    "    for (func, params) in vectorizer_list:\n",
    "        applied_vec.append(func(**state.process_params(params)))\n",
    "    \n",
    "    # Running the pipeline\n",
    "\n",
    "    if state.has_trained_model == False:\n",
    "        state.pipe = myPipeline(*applied_vec, classifier = state.model(C = state.C_range[state.C_idx], max_iter = state.max_iter_possibilities[state.max_iter_idx]), feature_selection = \"SelectKBest\", k_selectKBest = state.selectkbest_k_possibilities[state.selectkbest_k_idx])\n",
    "        state.has_trained_model = True\n",
    "        state.pipe.fit(train_X, train_date, train_y)\n",
    "\n",
    "    # Evaluating the model\n",
    "\n",
    "    test_pred_y_probs = state.pipe.predict_proba(test_X, test_date)\n",
    "    pred_ids = np.argpartition(test_pred_y_probs, kth=-k, axis=1)\n",
    "    candidate_preds = pred_ids[:, -k:]\n",
    "\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    l3 = []\n",
    "    l4 = []\n",
    "    l5 = []\n",
    "    l6 = []\n",
    "\n",
    "    # Generate the csv file\n",
    "\n",
    "    for e in range(len(candidate_preds)):\n",
    "        l1.append(e)\n",
    "        l2.append(test_X[e])\n",
    "        l3.append(test_date[e])\n",
    "        l4.append(candidate_preds[e][-1])\n",
    "        l5.append(candidate_preds[e])\n",
    "        l6.append(test_gold_y[e])\n",
    "\n",
    "    df = DataFrame({'Index' : l1, \"Text\" : l2, \"Date\" : l3, \"top1\" : l4, \"top5\" : l5, \"label\" : l6})\n",
    "\n",
    "    df.to_csv('results_linearsvm.csv', index = False)\n",
    "\n",
    "\n",
    "    # test_pred_y = [gold if gold in candidate_preds[e] else candidate_preds[e, -1] for e, gold in enumerate(test_gold_y)]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the best state's file\n",
    "\n",
    "generate_csv(best_state)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
